{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_page_urls(search_keyword, page=1):\n",
    "    \"\"\"\n",
    "    获取搜索结果页面的 URL。根据需要处理分页。\n",
    "    \"\"\"\n",
    "    search_url = f\"{base_url}/index.php?q=search&keys={search_keyword}&edit%5Btype%5D%5Bbgimage%5D=on&page={page}\"\n",
    "    return search_url\n",
    "\n",
    "def get_detail_page_links(soup):\n",
    "    \"\"\"\n",
    "    从搜索结果页面提取详细页面的链接。\n",
    "    \"\"\"\n",
    "    detail_links = []\n",
    "    for img in soup.find_all(\"img\", attrs = {'class':\"bgimage-thumb\"}):\n",
    "        parent = img.find_parent(\"a\")\n",
    "        if parent and parent.get(\"href\"):\n",
    "            detail_page_url = urljoin(base_url, parent[\"href\"])\n",
    "            detail_links.append(detail_page_url)\n",
    "    return detail_links\n",
    "\n",
    "def get_high_res_image_url(soup):\n",
    "    \"\"\"\n",
    "    从详细页面提取高清图片的 URL。\n",
    "    根据实际页面结构调整选择器。\n",
    "    \"\"\"\n",
    "    imgs = soup.find_all(\"img\")\n",
    "    # 根据观察结果，调整提取逻辑\n",
    "    for img in imgs:\n",
    "        src = img.get(\"src\", \"\")\n",
    "        if \"images/cache/\" in src or \"images/raw/\" in src:\n",
    "            high_res_url = urljoin(base_url, src)\n",
    "            return high_res_url\n",
    "\n",
    "    print(\"未找到高清图片链接\")\n",
    "    return None\n",
    "\n",
    "def download_image(image_url, save_dir, idx):\n",
    "    \"\"\"\n",
    "    下载图片并保存到指定目录，图片名称按序号命名（例如 aphis_01, aphis_02）。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(image_url, headers=headers, stream=True)\n",
    "        response.raise_for_status()\n",
    "        # 生成新的图片名称，例如 aphis_001.jpg, aphis_002.jpg\n",
    "        image_name = f\"{search_keyword}_{idx:03d}.jpg\"  # 以三位数字格式命名\n",
    "        # 拼接保存路径\n",
    "        save_path = os.path.join(save_dir, image_name)\n",
    "        # 保存图片\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                f.write(chunk)\n",
    "        print(f\"已下载: {image_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"下载失败 {image_url}: {e}\")\n",
    "\n",
    "def main():\n",
    "    # 获取搜索结果的第一页\n",
    "    search_url = get_search_page_urls(search_keyword)\n",
    "    print(f\"请求搜索页面: {search_url}\")\n",
    "    resp = requests.get(search_url, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"无法访问搜索页面: 状态码 {resp.status_code}\")\n",
    "        return\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "    # 提取详细页面链接\n",
    "    detail_links = get_detail_page_links(soup)\n",
    "    print(f\"找到 {len(detail_links)} 个详细页面链接\")\n",
    "\n",
    "    downloaded_count = 0  # 记录下载的图片数量\n",
    "    for idx, detail_url in enumerate(detail_links, 1):\n",
    "        if downloaded_count >= 1000:  # 如果已经下载了500张图片，停止\n",
    "            print(\"已下载1000张图片，程序终止。\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            detail_resp = requests.get(detail_url, headers=headers)\n",
    "            if detail_resp.status_code != 200:\n",
    "                print(f\"无法访问详细页面: 状态码 {detail_resp.status_code}\")\n",
    "                continue\n",
    "            detail_soup = BeautifulSoup(detail_resp.text, \"lxml\")\n",
    "            high_res_url = get_high_res_image_url(detail_soup)\n",
    "            if high_res_url:\n",
    "                download_image(high_res_url, download_dir, downloaded_count + 1)  # 使用当前下载的图片数量\n",
    "                downloaded_count += 1  # 下载数量加1\n",
    "        except Exception as e:\n",
    "            print(f\"处理详细页面失败: {e}\")\n",
    "\n",
    "        # 为了避免被封，适当休眠\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置请求头，模拟浏览器访问\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' \\\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) ' \\\n",
    "                  'Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'\n",
    "}\n",
    "\n",
    "# 搜索关键词和基础 URL\n",
    "search_keyword = \"Aphis\"\n",
    "base_url = \"https://bugguide.net\"\n",
    "\n",
    "# 创建用于存储高清图片的目录\n",
    "download_dir = f\"{search_keyword}_images\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "def get_search_page_urls(search_keyword, page=1):\n",
    "    \"\"\"\n",
    "    获取搜索结果页面的 URL。根据需要处理分页。\n",
    "    \"\"\"\n",
    "    search_url = f\"{base_url}/index.php?q=search&keys={search_keyword}&edit%5Btype%5D%5Bbgimage%5D=on&page={page}\"\n",
    "    return search_url\n",
    "\n",
    "def get_detail_page_links(soup):\n",
    "    \"\"\"\n",
    "    从搜索结果页面提取详细页面的链接。\n",
    "    \"\"\"\n",
    "    detail_links = []\n",
    "    for img in soup.find_all(\"img\", attrs = {'class':\"bgimage-thumb\"}):\n",
    "        parent = img.find_parent(\"a\")\n",
    "        if parent and parent.get(\"href\"):\n",
    "            detail_page_url = urljoin(base_url, parent[\"href\"])\n",
    "            detail_links.append(detail_page_url)\n",
    "    return detail_links\n",
    "\n",
    "def get_high_res_image_url(soup):\n",
    "    \"\"\"\n",
    "    从详细页面提取高清图片的 URL。\n",
    "    根据实际页面结构调整选择器。\n",
    "    \"\"\"\n",
    "\n",
    "    imgs = soup.find_all(\"img\")\n",
    "    # print(f\"在详细页面中找到 {len(imgs)} 个 <img> 标签\")\n",
    "    for img in imgs:\n",
    "        src = img.get(\"src\", \"\")\n",
    "        # classes = img.get(\"class\", [])\n",
    "        # img_id = img.get(\"id\", \"\")\n",
    "        # print(f\"图片 src: {src}, class: {classes}, id: {img_id}\")\n",
    "\n",
    "    # 根据观察结果，调整提取逻辑\n",
    "    for img in imgs:\n",
    "        src = img.get(\"src\", \"\")\n",
    "        if \"images/cache/\" in src or \"images/raw/\" in src:\n",
    "            high_res_url = urljoin(base_url, src)\n",
    "            # print(f\"找到高清图片链接: {high_res_url}\")\n",
    "            return high_res_url\n",
    "\n",
    "    print(\"未找到高清图片链接\")\n",
    "    return None\n",
    "\n",
    "def download_image(image_url, save_dir, idx):\n",
    "    \"\"\"\n",
    "    下载图片并保存到指定目录，图片名称按序号命名（例如 aphis_01, aphis_02）。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(image_url, headers=headers, stream=True)\n",
    "        response.raise_for_status()\n",
    "        # 生成新的图片名称，例如 aphis_001.jpg, aphis_002.jpg\n",
    "        image_name = f\"aphis_{idx:03d}.jpg\"  # 以三位数字格式命名\n",
    "        # 拼接保存路径\n",
    "        save_path = os.path.join(save_dir, image_name)\n",
    "        # 保存图片\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                f.write(chunk)\n",
    "        print(f\"已下载: {image_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"下载失败 {image_url}: {e}\")\n",
    "\n",
    "def main():\n",
    "    # 获取搜索结果的第一页\n",
    "    search_url = get_search_page_urls(search_keyword)\n",
    "    print(f\"请求搜索页面: {search_url}\")\n",
    "    resp = requests.get(search_url, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"无法访问搜索页面: 状态码 {resp.status_code}\")\n",
    "        return\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "    # 提取详细页面链接\n",
    "    detail_links = get_detail_page_links(soup)\n",
    "    print(f\"找到 {len(detail_links)} 个详细页面链接\")\n",
    "\n",
    "    for idx, detail_url in enumerate(detail_links, 1):\n",
    "        # print(f\"\\n处理详细页面 {idx}/{len(detail_links)}: {detail_url}\")\n",
    "        try:\n",
    "            detail_resp = requests.get(detail_url, headers=headers)\n",
    "            if detail_resp.status_code != 200:\n",
    "                print(f\"无法访问详细页面: 状态码 {detail_resp.status_code}\")\n",
    "                continue\n",
    "            detail_soup = BeautifulSoup(detail_resp.text, \"lxml\")\n",
    "            high_res_url = get_high_res_image_url(detail_soup)\n",
    "            if high_res_url:\n",
    "                download_image(high_res_url, download_dir, idx)\n",
    "        except Exception as e:\n",
    "            print(f\"处理详细页面失败: {e}\")\n",
    "\n",
    "        # 为了避免被封，适当休眠\n",
    "        time.sleep(1)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置请求头，模拟浏览器访问\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' \\\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) ' \\\n",
    "                  'Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'\n",
    "}\n",
    "\n",
    "# 搜索关键词和基础 URL\n",
    "search_keyword = \"midge\"\n",
    "base_url = \"https://bugguide.net\"\n",
    "\n",
    "# 创建用于存储高清图片的目录\n",
    "download_dir = f\"{search_keyword}_images\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "def get_search_page_urls(search_keyword, page=1):\n",
    "    \"\"\"\n",
    "    获取搜索结果页面的 URL。根据需要处理分页。\n",
    "    \"\"\"\n",
    "    search_url = f\"{base_url}/index.php?q=search&keys={search_keyword}&edit%5Btype%5D%5Bbgimage%5D=on&page={page}\"\n",
    "    return search_url\n",
    "\n",
    "def get_detail_page_links(soup):\n",
    "    \"\"\"\n",
    "    从搜索结果页面提取详细页面的链接。\n",
    "    \"\"\"\n",
    "    detail_links = []\n",
    "    for img in soup.find_all(\"img\", attrs = {'class':\"bgimage-thumb\"}):\n",
    "        parent = img.find_parent(\"a\")\n",
    "        if parent and parent.get(\"href\"):\n",
    "            detail_page_url = urljoin(base_url, parent[\"href\"])\n",
    "            detail_links.append(detail_page_url)\n",
    "    return detail_links\n",
    "\n",
    "def get_high_res_image_url(soup):\n",
    "    \"\"\"\n",
    "    从详细页面提取高清图片的 URL。\n",
    "    根据实际页面结构调整选择器。\n",
    "    \"\"\"\n",
    "    imgs = soup.find_all(\"img\")\n",
    "    # 根据观察结果，调整提取逻辑\n",
    "    for img in imgs:\n",
    "        src = img.get(\"src\", \"\")\n",
    "        if \"images/cache/\" in src or \"images/raw/\" in src:\n",
    "            high_res_url = urljoin(base_url, src)\n",
    "            return high_res_url\n",
    "\n",
    "    print(\"未找到高清图片链接\")\n",
    "    return None\n",
    "\n",
    "def download_image(image_url, save_dir, idx):\n",
    "    \"\"\"\n",
    "    下载图片并保存到指定目录，图片名称按序号命名（例如 aphis_01, aphis_02）。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(image_url, headers=headers, stream=True)\n",
    "        response.raise_for_status()\n",
    "        # 生成新的图片名称，例如 aphis_001.jpg, aphis_002.jpg\n",
    "        image_name = f\"{search_keyword}_{idx:03d}.jpg\"  # 以三位数字格式命名\n",
    "        # 拼接保存路径\n",
    "        save_path = os.path.join(save_dir, image_name)\n",
    "        # 保存图片\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                f.write(chunk)\n",
    "        print(f\"已下载: {image_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"下载失败 {image_url}: {e}\")\n",
    "\n",
    "def main():\n",
    "    # 获取搜索结果的第一页\n",
    "    search_url = get_search_page_urls(search_keyword)\n",
    "    print(f\"请求搜索页面: {search_url}\")\n",
    "    resp = requests.get(search_url, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"无法访问搜索页面: 状态码 {resp.status_code}\")\n",
    "        return\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "    # 提取详细页面链接\n",
    "    detail_links = get_detail_page_links(soup)\n",
    "    print(f\"找到 {len(detail_links)} 个详细页面链接\")\n",
    "\n",
    "    downloaded_count = 0  # 记录下载的图片数量\n",
    "    for idx, detail_url in enumerate(detail_links, 1):\n",
    "        if downloaded_count >= 500:  # 如果已经下载了500张图片，停止\n",
    "            print(\"已下载500张图片，程序终止。\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            detail_resp = requests.get(detail_url, headers=headers)\n",
    "            if detail_resp.status_code != 200:\n",
    "                print(f\"无法访问详细页面: 状态码 {detail_resp.status_code}\")\n",
    "                continue\n",
    "            detail_soup = BeautifulSoup(detail_resp.text, \"lxml\")\n",
    "            high_res_url = get_high_res_image_url(detail_soup)\n",
    "            if high_res_url:\n",
    "                download_image(high_res_url, download_dir, downloaded_count + 1)  # 使用当前下载的图片数量\n",
    "                downloaded_count += 1  # 下载数量加1\n",
    "        except Exception as e:\n",
    "            print(f\"处理详细页面失败: {e}\")\n",
    "\n",
    "        # 为了避免被封，适当休眠\n",
    "        time.sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置请求头，模拟浏览器访问\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' \\\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) ' \\\n",
    "                  'Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'\n",
    "}\n",
    "\n",
    "# 搜索关键词和基础 URL\n",
    "search_keyword = \"Armyworm\"\n",
    "base_url = \"https://bugguide.net\"\n",
    "\n",
    "# 创建用于存储高清图片的目录\n",
    "download_dir = f\"{search_keyword}_images\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置请求头，模拟浏览器访问\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' \\\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) ' \\\n",
    "                  'Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'\n",
    "}\n",
    "\n",
    "# 搜索关键词和基础 URL\n",
    "search_keyword = \"Wireworm\"\n",
    "base_url = \"https://bugguide.net\"\n",
    "\n",
    "# 创建用于存储高清图片的目录\n",
    "download_dir = f\"{search_keyword}_images\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置请求头，模拟浏览器访问\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' \\\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) ' \\\n",
    "                  'Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'\n",
    "}\n",
    "\n",
    "# 搜索关键词和基础 URL\n",
    "search_keyword = \"Sawfly\"\n",
    "base_url = \"https://bugguide.net\"\n",
    "\n",
    "# 创建用于存储高清图片的目录\n",
    "download_dir = f\"{search_keyword}_images\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置请求头，模拟浏览器访问\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' \\\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) ' \\\n",
    "                  'Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'\n",
    "}\n",
    "\n",
    "# 搜索关键词和基础 URL\n",
    "search_keyword = \"Thrips\"\n",
    "base_url = \"https://bugguide.net\"\n",
    "\n",
    "# 创建用于存储高清图片的目录\n",
    "download_dir = f\"{search_keyword}_images\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置请求头，模拟浏览器访问\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' \\\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) ' \\\n",
    "                  'Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0'\n",
    "}\n",
    "\n",
    "# 搜索关键词和基础 URL\n",
    "search_keyword = \"Mite\"\n",
    "base_url = \"https://bugguide.net\"\n",
    "\n",
    "# 创建用于存储高清图片的目录\n",
    "download_dir = f\"{search_keyword}_images\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
